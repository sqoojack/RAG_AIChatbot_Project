# Answering_Interface.py
from utils import os, json, st 
from langchain.schema import Document  
from UI_model_select import render_model_settings_ui  
from RAG_Embedding import load_vectorstore
from RAG_LLM_Generator import llm_generator, extract_answer_and_thought 
from reranking import search_top_k, rerank_chunks_top_k  
from full_file import generate_full_files_answer  # Custom RAG è™•ç†é‚è¼¯
from config import default_model_settings, ollama_url, VECTOR_DB_DIR, reranking_url, reranking_api, cert_datapath  

# è¨­å®š Streamlit é é¢è³‡è¨Š
st.set_page_config(page_title="å•ç­”ä»‹é¢", page_icon="ğŸ’¬")
st.title("ğŸ’¬ å•ç­”ä»‹é¢")

# åˆå§‹åŒ– model_settingsï¼ˆå¾ Session State å–å¾—æˆ–å¥—ç”¨é è¨­å€¼ï¼‰
if "model_settings" not in st.session_state:
    st.session_state.model_settings = default_model_settings.copy()

# åˆå§‹åŒ–å‘é‡è³‡æ–™åº«èˆ‡æ–‡ä»¶åˆ—è¡¨
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "docs" not in st.session_state:
    st.session_state.docs = []

# æœå°‹æœ¬åœ°è³‡æ–™å¤¾ä¸­æ‰€æœ‰å‘é‡è³‡æ–™åº«ï¼ˆè³‡æ–™å¤¾åç¨± = DB åç¨±ï¼‰
vector_db_names = [f for f in os.listdir(VECTOR_DB_DIR) if os.path.isdir(os.path.join(VECTOR_DB_DIR, f))]

# ä½¿ç”¨ selectbox è®“ä½¿ç”¨è€…é¸æ“‡ä¸€å€‹è³‡æ–™åº«
selected_db = st.selectbox("ğŸ“‚ **é¸æ“‡å·²å»ºç«‹çš„å‘é‡è³‡æ–™åº«**", options=vector_db_names)

# å¦‚æœä½¿ç”¨è€…æœ‰é¸æ“‡è³‡æ–™åº«ï¼Œå‰‡å˜—è©¦è®€å–å‘é‡è³‡æ–™èˆ‡åŸå§‹æ–‡ä»¶
if selected_db:
    try:
        db_path = os.path.join(VECTOR_DB_DIR, selected_db)

        # è¼‰å…¥å‘é‡è³‡æ–™åº«ï¼ˆé€é Ollama çš„ Embedding Modelï¼‰
        vectorstore = load_vectorstore(
            db_path,
            embedding_model=st.session_state.model_settings.get("embedding_model", "bge-m3"),
        )
        st.session_state.vectorstore = vectorstore

        # è®€å–åŸå§‹æ–‡å­—å…§å®¹ï¼ˆä¾‹å¦‚é é¢å…§å®¹èˆ‡ä¾†æºè³‡è¨Šï¼‰æ”¾å…¥ docs
        metadata_path = os.path.join(db_path, "metadata.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata_list = json.load(f)

            docs_list = []
            if isinstance(metadata_list, list):
                # metadata.json æ˜¯ List æ ¼å¼
                for meta in metadata_list:
                    page_content = meta.get("page_content", "")
                    meta_data = meta.get("metadata", {})
                    docs_list.append(Document(page_content=page_content, metadata=meta_data))
            elif isinstance(metadata_list, dict):
                # metadata.json æ˜¯å–®ä¸€ Document æ ¼å¼
                page_content = metadata_list.get("page_content", "")
                meta_data = metadata_list.get("metadata", {})
                docs_list.append(Document(page_content=page_content, metadata=meta_data))
            st.session_state.docs = docs_list
        else:
            st.session_state.docs = []

    except Exception as e:
        st.error(f"âŒ å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—ï¼š{e}")
        st.stop()
else:
    st.warning("âš ï¸ å°šæœªå»ºç«‹å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆè‡³ã€å»ºç«‹çŸ¥è­˜åº«ã€‘é ä¸Šå‚³æª”æ¡ˆã€‚")
    st.stop()

# é¡¯ç¤º LLM æ¨¡å‹èˆ‡åƒæ•¸è¨­å®šé¸å–®ï¼ˆtop_pã€temperature ç­‰ï¼‰
render_model_settings_ui()

# ä½¿ç”¨è€…è¼¸å…¥å•é¡Œ
query = st.text_area("**è«‹è¼¸å…¥ä½ çš„å•é¡Œ**", value="PRODå‚™ä»½åˆ°CLONE5ä¹‹å¾Œè¦åšå“ªäº›å·¥ä½œé …ç›®?")

# å•Ÿå‹•å•ç­”æµç¨‹
if st.button("å›ç­”å•é¡Œ"):
    if not query.strip():
        st.warning("è«‹è¼¸å…¥å•é¡Œ")
    else:
        try:
            with st.spinner("æª¢ç´¢ä¸­..."):
                # æ ¹æ“šè¨­å®šé¸æ“‡æœå°‹æ–¹æ³•ï¼šBasic / Reranking / MMR / Custom RAG
                search_method = st.session_state.model_settings.get("search_method", "Basic")
                vectorstore = st.session_state.vectorstore

                if search_method == "MMR":
                    # æ¡ç”¨ Maximal Marginal Relevance æª¢ç´¢
                    retriever = vectorstore.as_retriever(
                        search_type="mmr",
                        search_kwargs={"k": st.session_state.model_settings.get("top_k", 5), "fetch_k": st.session_state.model_settings.get("top_n", 10), "lambda_mult": 0.5}
                    )
                    mmr_docs = retriever.invoke(query)
                    top_chunks = [(doc, 1.0) for doc in mmr_docs]
                    st.success("âœ… MMRæª¢ç´¢å®Œæˆ")

                elif search_method in ["Reranking", "Custom RAG"]:
                    # ä½¿ç”¨å‘é‡æœå°‹åˆæ­¥å–å¾— top_k å€™é¸ç‰‡æ®µ
                    candidates = search_top_k(query, vectorstore, top_k=st.session_state.model_settings.get("top_n", 10))
                    # å†é€é Reranker ç²¾é¸ top_k ç‰‡æ®µ
                    top_chunks = rerank_chunks_top_k(
                        query,
                        candidates,
                        top_k=st.session_state.model_settings.get("top_k", 5),
                        reranking_url=reranking_url,
                        reranking_api=reranking_api,
                        cert_datapath=cert_datapath
                    )
                    st.success("âœ… Rerankingå®Œæˆ")

                else:
                    # æ¡ç”¨æœ€åŸºæœ¬çš„å‘é‡ Top-K æœå°‹
                    top_chunks = search_top_k(
                        query,
                        vectorstore,
                        top_k=st.session_state.model_settings.get("top_k", 5)
                    )
                    st.success("âœ… Basic æª¢ç´¢å®Œæˆ")

            with st.spinner("ç”Ÿæˆå›ç­”ä¸­..."):
                if search_method == "Custom RAG":
                    # ä½¿ç”¨å®Œæ•´æª”æ¡ˆè³‡è¨Šçµåˆç‰‡æ®µåšå›ç­”ï¼ˆé©ç”¨é•·æ–‡ä»¶ï¼‰
                    file_chunks, answer = generate_full_files_answer(
                        top_chunks,
                        st.session_state.docs,
                        query,
                        ollama_url,
                        st.session_state.model_settings
                    )
                else:
                    # å‚³é€ query + top_chunks çµ¦ LLM æ¨¡å‹ç”Ÿæˆå›ç­”
                    answer = llm_generator(
                        query,
                        top_chunks,
                        ollama_url,
                        llm_model=st.session_state.model_settings.get("llm_model"),
                        temperature=st.session_state.model_settings.get("temperature", 0.0),
                        top_p=st.session_state.model_settings.get("top_p", 1.0),
                    )

                # åˆ†é›¢ LLM è¼¸å‡ºï¼šæœ€çµ‚ç­”æ¡ˆ vs æ€è€ƒéç¨‹
                ans, thought = extract_answer_and_thought(answer)

                # åˆ†ç‚ºå·¦å³å€å¡Šé¡¯ç¤ºå›ç­”èˆ‡ä¾†æºç‰‡æ®µ
                left, right = st.columns([2, 1])
                with left:
                    st.markdown("### ğŸ“˜ å›ç­”ï¼š")
                    st.markdown(ans)
                    if thought:
                        with st.expander("ğŸ’­ æ€è€ƒéç¨‹"):
                            st.markdown(thought)
                with right:
                    st.markdown("### ğŸ” åŒ¹é…æ®µè½ï¼š")
                    if search_method == "Custom RAG":
                        for i, doc in enumerate(file_chunks):
                            with st.expander(f"æ®µè½ {i+1} | ä¾†æº: {doc.metadata.get('source')}"):
                                st.write(doc.page_content)
                    else:
                        for i, (doc, score) in enumerate(top_chunks):
                            with st.expander(f"Rank {i+1} | ä¾†æº: {doc.metadata.get('source')} | åˆ†æ•¸: {score:.2f}"):
                                st.write(doc.page_content)

        except Exception as e:
            st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
